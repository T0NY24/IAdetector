[Unit]
Description=Semantic LLM Server - DeepSeek-R1 Microservice
After=network.target ollama.service
Requires=ollama.service

[Service]
Type=simple
User=root
WorkingDirectory=/opt/uide-forense/services
ExecStart=/usr/bin/python3 /opt/uide-forense/services/semantic_llm_server.py

# Environment variables
Environment="OLLAMA_API_URL=http://localhost:11434/api/generate"
Environment="OLLAMA_MODEL=deepseek-r1:7b"
Environment="LLM_SERVER_PORT=8000"
Environment="LLM_REQUEST_TIMEOUT=30"
Environment="LLM_MAX_RETRIES=3"
Environment="LOG_LEVEL=INFO"

# Restart policy
Restart=always
RestartSec=5
StartLimitBurst=5
StartLimitIntervalSec=60

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=llm-server

# Security (optional hardening)
# NoNewPrivileges=true
# PrivateTmp=true

[Install]
WantedBy=multi-user.target
