#!/bin/bash

###############################################################################
# Ollama + DeepSeek-R1:7b Installation Script
# UIDE Forense AI 3.0+ - VPS Setup
# 
# Target: Ubuntu 22.04, 24GB RAM, 200GB NVMe
# Purpose: Install Ollama and DeepSeek-R1:7b model for local LLM inference
###############################################################################

set -e  # Exit on error

echo "=========================================="
echo "üöÄ UIDE Forense AI - Ollama Setup"
echo "=========================================="
echo ""

# Check if running as root
if [ "$EUID" -ne 0 ]; then 
    echo "‚ùå Please run as root (use sudo)"
    exit 1
fi

# System requirements check
echo "üìä Checking system requirements..."
TOTAL_RAM=$(free -g | awk '/^Mem:/{print $2}')
if [ "$TOTAL_RAM" -lt 20 ]; then
    echo "‚ö†Ô∏è  Warning: System has ${TOTAL_RAM}GB RAM. Recommended: 24GB+"
fi

AVAILABLE_SPACE=$(df -BG / | awk 'NR==2 {print $4}' | sed 's/G//')
if [ "$AVAILABLE_SPACE" -lt 50 ]; then
    echo "‚ö†Ô∏è  Warning: Only ${AVAILABLE_SPACE}GB available. Model requires ~10GB."
fi

echo "‚úÖ RAM: ${TOTAL_RAM}GB, Disk: ${AVAILABLE_SPACE}GB available"
echo ""

# Update system
echo "üì¶ Updating system packages..."
apt-get update -qq
apt-get install -y curl wget git

# Install Ollama
echo ""
echo "üîß Installing Ollama..."
if command -v ollama &> /dev/null; then
    echo "‚úÖ Ollama already installed ($(ollama --version))"
else
    curl -fsSL https://ollama.com/install.sh | sh
    echo "‚úÖ Ollama installed successfully"
fi

# Start Ollama service
echo ""
echo "üîÑ Starting Ollama service..."
systemctl enable ollama
systemctl start ollama
sleep 3

# Verify Ollama is running
if systemctl is-active --quiet ollama; then
    echo "‚úÖ Ollama service is running"
else
    echo "‚ùå Failed to start Ollama service"
    exit 1
fi

# Download DeepSeek-R1:7b model
echo ""
echo "üì• Downloading DeepSeek-R1:7b model..."
echo "‚ö†Ô∏è  This may take 10-15 minutes (model size: ~4.7GB)"
echo ""

ollama pull deepseek-r1:7b

if [ $? -eq 0 ]; then
    echo ""
    echo "‚úÖ DeepSeek-R1:7b model downloaded successfully"
else
    echo "‚ùå Failed to download model"
    exit 1
fi

# Test the model
echo ""
echo "üß™ Testing DeepSeek-R1:7b model..."
TEST_RESPONSE=$(ollama run deepseek-r1:7b "Say 'OK' if you are ready" --verbose=false 2>&1 | head -n 1)

if [[ "$TEST_RESPONSE" == *"OK"* ]] || [[ "$TEST_RESPONSE" == *"ok"* ]]; then
    echo "‚úÖ Model test successful: $TEST_RESPONSE"
else
    echo "‚ö†Ô∏è  Model loaded but response unclear: $TEST_RESPONSE"
fi

# Configure Ollama to listen on localhost only (security)
echo ""
echo "üîí Configuring Ollama security settings..."

# Create Ollama environment file
cat > /etc/systemd/system/ollama.service.d/override.conf << 'EOF'
[Service]
Environment="OLLAMA_HOST=127.0.0.1:11434"
Environment="OLLAMA_ORIGINS=http://localhost:*,http://127.0.0.1:*"
EOF

# Reload systemd and restart Ollama
systemctl daemon-reload
systemctl restart ollama
sleep 2

# Verify API endpoint
echo ""
echo "üîç Verifying API endpoint..."
API_RESPONSE=$(curl -s http://localhost:11434/api/tags 2>&1)

if [[ "$API_RESPONSE" == *"deepseek-r1"* ]]; then
    echo "‚úÖ API endpoint responding correctly"
else
    echo "‚ö†Ô∏è  API endpoint check inconclusive"
fi

# Display system status
echo ""
echo "=========================================="
echo "‚úÖ INSTALLATION COMPLETE"
echo "=========================================="
echo ""
echo "üìã Summary:"
echo "  ‚Ä¢ Ollama service: $(systemctl is-active ollama)"
echo "  ‚Ä¢ API endpoint: http://localhost:11434"
echo "  ‚Ä¢ Model: DeepSeek-R1:7b"
echo "  ‚Ä¢ Service starts on boot: enabled"
echo ""
echo "üîß Useful commands:"
echo "  ‚Ä¢ Check service status: systemctl status ollama"
echo "  ‚Ä¢ View logs: journalctl -u ollama -f"
echo "  ‚Ä¢ Test model: ollama run deepseek-r1:7b"
echo "  ‚Ä¢ List models: ollama list"
echo ""
echo "üöÄ Next step: Deploy semantic_llm_server.py microservice"
echo ""
